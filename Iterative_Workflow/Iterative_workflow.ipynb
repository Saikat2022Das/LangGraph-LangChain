{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c341e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from pydantic import BaseModel, Field # This is needed for Structured Output from LLM Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cdb48d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "768fb918",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(\n",
    "    model=\"google_genai:gemini-2.5-flash-lite\",\n",
    "    temperature=0.6,\n",
    "    max_tokens=400,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4608d270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As an expert software engineer, I see Docker as a transformative tool with a wide array of practical applications that streamline development, deployment, and operations.\\n\\n**Development Environments:** Docker excels at creating consistent, isolated development environments. Developers can spin up pre-configured containers with specific language versions, databases, and dependencies, eliminating the \"it works on my machine\" problem. This ensures everyone on the team uses the exact same setup, reducing integration headaches.\\n\\n**Application Deployment:** Docker containers package applications and their dependencies into portable units. This makes deployment incredibly simple and repeatable across different environments â€“ from a developer\\'s laptop to staging servers and production. The immutability of containers means you deploy the exact same artifact every time, leading to greater stability and predictability.\\n\\n**Microservices Architecture:** Docker is a natural fit for microservices. Each service can be containerized independently, allowing teams to develop, deploy, and scale them autonomously. This promotes agility and resilience, as a failure in one service doesn\\'t necessarily bring down the entire application.\\n\\n**Testing and CI/CD:** Docker simplifies testing by providing clean, reproducible environments for running tests. Continuous Integration/Continuous Deployment (CI/CD) pipelines leverage Docker to build, test, and deploy applications automatically, accelerating release cycles and improving software quality.\\n\\n**Legacy Application Modernization:** Docker can be used to \"containerize\" legacy applications, making them easier to manage, scale, and migrate to modern infrastructure without significant code rewrites. This provides a pathway to modernization without immediate disruption.\\n\\nIn essence, Docker provides a standardized way to build, ship, and run applications, unlocking efficiency, portability, and scalability across the entire software lifecycle.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"You are an expert software engineer. What are real usees of Docker, in 300 words?\"\"\"\n",
    "model.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b204273",
   "metadata": {},
   "source": [
    "### We need 3 Different LLM,\n",
    "#### 1st: Generate Twitter Post\n",
    "#### 2nd: Evaluation of Generated Post\n",
    "#### 3rd: Optimize the Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d078c8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_llm = model\n",
    "evaluator_llm = model\n",
    "optimizer_llm = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a506c5",
   "metadata": {},
   "source": [
    "#### State of Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "80a759dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetState(TypedDict):\n",
    "\n",
    "    topic: str\n",
    "    tweet: str\n",
    "    evaluation: Literal[\"Approved\", \"Needs_Improvement\"]\n",
    "    feedback: str\n",
    "\n",
    "    iteration: int\n",
    "    max_iteration: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "81d7e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b5aab593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tweet(state: TweetState):\n",
    "\n",
    "    # prompt\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a funny and clever Twitter/X influencer.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "Write a short, original, and hilarious tweet on the topic: \"{state['topic']}\".\n",
    "\n",
    "Rules:\n",
    "- Do NOT use question-answer format.\n",
    "- Max 280 characters.\n",
    "- Use observational humor, irony, sarcasm, or cultural references.\n",
    "- Think in meme logic, punchlines, or relatable takes.\n",
    "- Use simple, day to day english\n",
    "\"\"\")\n",
    "    ]\n",
    "\n",
    "    # Send to LLM\n",
    "    tweet = generator_llm.invoke(messages).content\n",
    "\n",
    "    return {\n",
    "        \"tweet\" : tweet\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82e085a",
   "metadata": {},
   "source": [
    "#### We have to make schema for structured output for Evaluation model, like only \"Approved\" or \"Need Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e9e09c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class evaluationSchema(BaseModel):\n",
    "\n",
    "    evaluation: Literal['approved', 'needs_improvement'] = Field(description=\"The evaluation result of the tweet.\")\n",
    "    feedback: str = Field(description=\"Feedback on the tweet's performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb053c",
   "metadata": {},
   "source": [
    "#### Maked structured Evaluation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e4c13331",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_llm_structured = evaluator_llm.with_structured_output(evaluationSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "751e06de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tweet(state: TweetState):\n",
    "\n",
    "    # Prompt\n",
    "    messages = [\n",
    "    SystemMessage(content=\"You are a ruthless, no-laugh-given Twitter critic. You evaluate tweets based on humor, originality, virality, and tweet format.\"),\n",
    "    HumanMessage(content=f\"\"\"\n",
    "Evaluate the following tweet:\n",
    "\n",
    "Tweet: \"{state['tweet']}\"\n",
    "\n",
    "Use the criteria below to evaluate the tweet:\n",
    "\n",
    "1. Originality â€“ Is this fresh, or have you seen it a hundred times before?  \n",
    "2. Humor â€“ Did it genuinely make you smile, laugh, or chuckle?  \n",
    "3. Punchiness â€“ Is it short, sharp, and scroll-stopping?  \n",
    "4. Virality Potential â€“ Would people retweet or share it?  \n",
    "5. Format â€“ Is it a well-formed tweet (not a setup-punchline joke, not a Q&A joke, and under 280 characters)?\n",
    "\n",
    "Auto-reject if:\n",
    "- It's written in question-answer format (e.g., \"Why did...\" or \"What happens when...\")\n",
    "- It exceeds 280 characters\n",
    "- It reads like a traditional setup-punchline joke\n",
    "- Dont end with generic, throwaway, or deflating lines that weaken the humor (e.g., â€œMasterpieces of the auntie-uncle universeâ€ or vague summaries)\n",
    "\n",
    "### Respond ONLY in structured format:\n",
    "- evaluation: \"approved\" or \"needs_improvement\"  \n",
    "- feedback: One paragraph explaining the strengths and weaknesses \n",
    "\"\"\")\n",
    "]\n",
    "\n",
    "    # Evaluation Criteria => [Approved, Needs_Improvement] so we need a structured output.\n",
    "\n",
    "    evaluation = evaluator_llm_structured.invoke(messages)\n",
    "\n",
    "    return {\n",
    "        'evaluation' : evaluation.evaluation,\n",
    "        'feedback' : evaluation.feedback\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_tweet(state: TweetState):\n",
    "\n",
    "    # Prompt\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You punch up tweets for virality and humor based on given feedback.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "Improve the tweet based on this feedback:\n",
    "\"{state['feedback']}\"\n",
    "\n",
    "Topic: \"{state['topic']}\"\n",
    "Original Tweet:\n",
    "{state['tweet']}\n",
    "\n",
    "Re-write it as a short, viral-worthy tweet. Avoid Q&A style and stay under 280 characters.\n",
    "\"\"\")\n",
    "    ]\n",
    "\n",
    "    # Send to LLM\n",
    "    optimized_tweet = optimizer_llm.invoke(messages).content\n",
    "\n",
    "    iteration = state['iteration'] + 1\n",
    "\n",
    "    # return optimized tweet\n",
    "    return {\n",
    "        \"tweet\": optimized_tweet,\n",
    "        \"iteration\": iteration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e9aa9c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_evaluation(state: TweetState):\n",
    "\n",
    "    if state['evaluation'] == \"Approved\" or state['iteration'] >= state['max_iteration']:\n",
    "        return \"approved\"\n",
    "    return \"need_improvements\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "63307e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(TweetState)\n",
    "\n",
    "graph.add_node(\"Generate\", generate_tweet)\n",
    "graph.add_node(\"Evaluate\", evaluate_tweet)\n",
    "graph.add_node(\"Optimize\", optimize_tweet)\n",
    "\n",
    "graph.add_edge(START, \"Generate\")\n",
    "graph.add_edge(\"Generate\", \"Evaluate\")\n",
    "\n",
    "graph.add_conditional_edges(\"Evaluate\", route_evaluation, {'approved': END, 'need_improvements': \"Optimize\"})\n",
    "graph.add_edge(\"Optimize\", \"Evaluate\")\n",
    "\n",
    "workflow = graph.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "670728b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x7007289335f0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6dfb4e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing tweet with feedback: The tweet attempts a topical joke about AI and elections, but it falls flat. The humor relies on a predictable trope of bribing algorithms, and the rosogolla reference feels forced rather than genuinely funny. It's not particularly original, and the punchline is weak, lacking the sharpness needed for virality. While it's under the character limit and not a Q&A, the overall execution needs significant improvement to be considered approved.\n",
      "Optimized tweet: Here are a few options, playing with different angles to make it funnier and more viral:\n",
      "\n",
      "**Option 1 (Focus on the absurdity):**\n",
      "\n",
      "> Heard AI is predicting Bengal election results. My prediction? It's gonna say \"Mamata Banerjee wins... if you feed the AI enough rosogollas. It's a sweet deal, apparently.\" ðŸ¬ #BengalElections #AI\n",
      "\n",
      "**Option 2 (More direct and punchy):**\n",
      "\n",
      "> West Bengal elections are so advanced, AI's predicting results. Pretty sure it'll just spit out: \"Didi wins, but the algorithm demands a *sweet* bribe of rosogollas.\" #BengalElections #AI #KhelaHobe\n",
      "\n",
      "**Option 3 (Slightly more self-aware humor):**\n",
      "\n",
      "> So AI is predicting Bengal election outcomes now. My guess is it'll be like, \"Didi wins... provided she appeases the algorithm with a generous offering of rosogollas. The future is delicious.\" ðŸ¤£ #BengalElections #AI\n",
      "\n",
      "**Why these are better:**\n",
      "\n",
      "*   **Less predictable setup:** Instead of \"My money's on it saying,\" we're framing it as a more direct prediction or a commentary on the AI's supposed needs.\n",
      "*   **Rosogolla integration:** The rosogolla bribe is presented as the *reason* or the *condition* for the win, making it a more integral part of the punchline, not just tacked on.\n",
      "*   **Stronger punchline:** The humor comes from the absurdity of a digital AI needing a physical, sweet bribe.\n",
      "*   **Emojis:** A well-placed emoji adds visual appeal and reinforces the humor.\n",
      "*   **Conciseness:** They're all well under the character limit and get to the joke quickly.\n",
      "\n",
      "Choose the one that best fits your comedic style!\n",
      "Optimizing tweet with feedback: The user has provided multiple options for a tweet rather than a single tweet to evaluate. The core joke about AI predicting Bengal election results and requiring rosogollas as a bribe is somewhat derivative, playing on the common trope of AI needing specific inputs or rewards. While the rosogolla angle adds a local flavor, the humor doesn't land with a strong punch. The options are also presented in a way that feels more like a suggestion list than a standalone tweet, and the meta-commentary about why they are better detracts from the tweet itself. The format is also a bit too long and explanatory for a typical tweet evaluation.\n",
      "Optimized tweet: AI predicting Bengal election results? My money's on it saying \"Didi wins... if you bribe the algorithm with enough rosogollas. Apparently, the future is sweet. ðŸ¬ #BengalElections #AI\"\n",
      "Optimizing tweet with feedback: The tweet attempts humor by combining AI predictions with a cultural reference (rosogollas), but the joke falls flat. The originality is questionable, as AI and election predictions are common topics. The humor isn't strong enough to elicit a laugh, and the punchline feels a bit forced. While it's under the character limit and not a Q&A, the ending line, \"Apparently, the future is sweet. ðŸ¬\", acts as a weak deflator rather than a strong punch. It lacks the sharpness needed for virality.\n",
      "Optimized tweet: Here are a few options, playing with different angles for humor and punch:\n",
      "\n",
      "**Option 1 (More Absurd):**\n",
      "\n",
      "> AI's predicting Bengal election results. My money's on it saying Didi wins, but only if you feed the algorithm a lifetime supply of rosogollas. The future of democracy is... surprisingly sugary. ðŸ¬ #BengalElections #AI\n",
      "\n",
      "**Option 2 (Slightly More Cynical):**\n",
      "\n",
      "> So AI's predicting Bengal election results. My guess? It'll declare Didi the winner, but only after a \"processing fee\" paid in rosogollas. Guess that's how the future sweetens the deal. ðŸ¬ #BengalElections #AI\n",
      "\n",
      "**Option 3 (Focus on the \"Sweetener\"):**\n",
      "\n",
      "> AI's crunching Bengal election numbers. My bet: It'll announce Didi's victory, but with a crucial footnote: \"Results may vary based on rosogolla intake.\" The future is, uh, *sweetened*. ðŸ¬ #BengalElections #AI\n",
      "\n",
      "**Why these are punchier:**\n",
      "\n",
      "*   **Stronger Punchlines:** Instead of \"the future is sweet,\" they connect the rosogollas more directly to the *outcome* or the *process* of prediction, making it a more active part of the joke.\n",
      "*   **More Active Verbs/Phrasing:** \"Feed the algorithm,\" \"processing fee,\" \"crucial footnote\" create more vivid imagery.\n",
      "*   **Sharper Tone:** The slight cynicism or absurdity lands better than a simple statement.\n",
      "*   **Conciseness:** Trimmed unnecessary words.\n",
      "\n",
      "Choose the one that best fits the vibe you're going for!\n",
      "Optimizing tweet with feedback: The tweet is auto-rejected because it exceeds the 280-character limit and is presented as a meta-commentary on joke construction rather than a joke itself. The core joke about AI and rosogollas is decent, but the presentation as multiple options and the lengthy explanation dilute any potential humor and virality. The suggestions themselves are not punchy enough to stand alone as effective tweets.\n"
     ]
    },
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Error calling model 'gemini-2.5-flash-lite' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash-lite\\nPlease retry in 7.240106469s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '10'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '7s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:3040\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   3039\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3040\u001b[39m     response: GenerateContentResponse = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3041\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3042\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/google/genai/models.py:5215\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5214\u001b[39m i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5215\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5217\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5219\u001b[39m function_map = _extra_utils.get_function_map(parsed_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/google/genai/models.py:3997\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   3995\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m3997\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3998\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   3999\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   4002\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4003\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/google/genai/_api_client.py:1375\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1372\u001b[39m http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1373\u001b[39m     http_method, path, request_dict, http_options\n\u001b[32m   1374\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1375\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1376\u001b[39m response_body = (\n\u001b[32m   1377\u001b[39m     response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1378\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/google/genai/_api_client.py:1209\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1208\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1209\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m   1211\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/tenacity/__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/tenacity/__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/google/genai/_api_client.py:1188\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1181\u001b[39m response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m   1182\u001b[39m     method=http_request.method,\n\u001b[32m   1183\u001b[39m     url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1186\u001b[39m     timeout=http_request.timeout,\n\u001b[32m   1187\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1188\u001b[39m \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1190\u001b[39m     response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1191\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/google/genai/errors.py:121\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    119\u001b[39m   response_json = response.body_segments[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/google/genai/errors.py:146\u001b[39m, in \u001b[36mAPIError.raise_error\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n",
      "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash-lite\\nPlease retry in 7.240106469s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '10'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '7s'}]}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m initial_state = {\n\u001b[32m      2\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtopic\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mAI in West Bengal Elections\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33miteration\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmax_iteration\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m5\u001b[39m\n\u001b[32m      5\u001b[39m }\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m result = \u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/langgraph/pregel/main.py:3068\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3065\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3066\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3068\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/langgraph/pregel/main.py:2643\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2641\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2642\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2643\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2653\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36moptimize_tweet\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Send to LLM\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOptimizing tweet with feedback:\u001b[39m\u001b[33m\"\u001b[39m, state[\u001b[33m'\u001b[39m\u001b[33mfeedback\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m optimized_tweet = \u001b[43moptimizer_llm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m.content\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOptimized tweet:\u001b[39m\u001b[33m\"\u001b[39m, optimized_tweet)\n\u001b[32m     23\u001b[39m iteration = state[\u001b[33m'\u001b[39m\u001b[33miteration\u001b[39m\u001b[33m'\u001b[39m] + \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:2535\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.invoke\u001b[39m\u001b[34m(self, input, config, code_execution, stop, **kwargs)\u001b[39m\n\u001b[32m   2532\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33mTools are already defined.code_execution tool can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be defined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2533\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m-> \u001b[39m\u001b[32m2535\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1233\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1237\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:3044\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   3040\u001b[39m     response: GenerateContentResponse = \u001b[38;5;28mself\u001b[39m.client.models.generate_content(\n\u001b[32m   3041\u001b[39m         **request,\n\u001b[32m   3042\u001b[39m     )\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m3044\u001b[39m     \u001b[43m_handle_client_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangGraph/Iterative_Workflow/myenv/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:145\u001b[39m, in \u001b[36m_handle_client_error\u001b[39m\u001b[34m(e, request)\u001b[39m\n\u001b[32m    143\u001b[39m model_name = request.get(\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    144\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError calling model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.status\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m: Error calling model 'gemini-2.5-flash-lite' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash-lite\\nPlease retry in 7.240106469s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '10'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '7s'}]}}",
      "During task with name 'Optimize' and id '88f13e33-0e2f-f078-b8bb-730d25505897'"
     ]
    }
   ],
   "source": [
    "initial_state = {\n",
    "    'topic': 'AI in West Bengal Elections',\n",
    "    'iteration': 1,\n",
    "    'max_iteration': 5\n",
    "}\n",
    "\n",
    "result = workflow.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b57b22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
